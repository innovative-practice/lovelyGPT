信息与通信导论复习
Edit by Seaky.

1

复习范围

考试内容不超过PPT
不考证明
1.1

编码与汉明码
1. 现在编码是什么样
2. 这个编码的汉明码是什么样

译码

3. 给出一个有错误的汉明码，使用汉明码进行

1.2

第三周
1. 舰艇游戏——验证信息量的相关性
2. 香农信息量
3. 二进制熵函数（提过需要了解）
(a)

性质

4. 称重游戏

1.3

(a)

本质是策略

(b)

如何计算每个策略的熵值大小

第四周
1. 给定一个待编码的集合，需要用多少比特编码（需要知道）
2. 压缩：
(a)

有损压缩器失效概率 $

有100个码字，只想表示其中80个码字，计算

3. 信源压缩方案——硬币彩票
(a)

思想

4. 典型集
(a)

记住概念

5. 香农信源编码原理

1.4

(a)

记住概念

(b)

要会计算

第五周——符号码
1. 导引
(a)

唯一可译码性：克拉夫特不等式

(b)

前缀码

(c)

什么样的符号码是最优的
i. 理论最优编码
ii. 实际操作最优编码
iii. 二者区别

(d)

霍夫曼编码（比较重要，要会算）

2. 评价编码
(a)

编码准确性：断是否是唯一可译码的

(b)

编码简单性：编码是否简单

(c)

编码长度简短性：期望编码长度尽可能短

3. 唯一可译码性
(a)

前缀码

(b)

当克拉夫特不等式等号为1时——完全码

4. 理论上编码长度下界
5. 实际场景最优编码长度
6. 霍夫曼编码——一定要会算

1.5

第六周——算术码
1. 霍夫曼算法的缺点

例子都要会算）

2. 算术码（
(a)

什么是算术码、特点

(b)

缺点

概率小区间的计算公式

(c)

1.6

第八周

必须要会算）
2. 条件熵，联合熵，互信息（必须要会算）
1. 贝叶斯定理（
3. 熵的全景图

重点）

4. 给定信道，计算信道容量、互信息（

1.7

第十周
1. 决策树
(a)

原理——互信息计算

2. 互信息——基于Y推断X

看一下）

(a)

二进制对称信道案例互信息熵计算（

(b)

计算：第二种比第一种简单一些

必须知道）

3. 信道容量（

1.8

(a)

输入信号的最优分布是什么

(b)

例子

第十二周
1. 信道对应的是一个条件概率
2. 有噪声的信道传输（有噪声打字机）和无误差时的通信的互信息是等价的
(a)

一些理解（看一下了解下就行）

1.9

第十三周（老师说全都过一遍就行）
1. 贝叶斯推断（老师说看一遍就行）

1.10

(a)

最大化似然函数

(b)

求导

第十四周
1. 蒙特卡洛

2

(a)

计算积分

(b)

接受-拒绝采样法

(c)

重要采样

信息系统，熵，重复码，汉明码
通信的目的：实现信息的保真传输

信道编码器：保障可靠性，检验码等技术
调制：将数字信号转化为模拟信号
信息系统的传输指标：

有效性
传输的可靠性
传输的安全性
传输的

对应数字通信的三项基本技术：
数据压缩技术
数据纠错技术
数据加密技术
信息论的研究内容：
狭义信息论
–

香农信息论，或是经典信息论

–

除了研究什么是信息、如何度量信息、信道容量等，主要回答三个基本问题

◦

香农第一定理：为了无失真地传输信源信息，信源编码所需的最少码符号数是多
少？

香农第二定理：在有噪声信道中，能够以多大的速率可靠的传输数据？
◦ 香农第三定理：如允许一定量的失真，信源编码所需的最少码符号数是多少？
◦

2.1

信息量与信息熵
四个基本问题：
1. 随机性与概率的关系
信息是对不确定性的消除，消除的越多，获得的信息量就越大

2. 概率为1的事件的信息量
概率为1的事件的信息量为0

3. 概率为0的事件的信息量
概率为0的���件的信息量为一个很大的值

4. 两个独立事件的联合信息量
如果 ， 为独立事件，则

香农信息量（单位： ）：

由此可得一个信息的

定义：一个离散型随机变量 的熵

定义为：

2.1.1

问题

Q1：均匀硬币，出现“正面”的信息量是多少？
Answer：
出现正面的概率为

，则信息量为：

Q2：从52张扑克牌中，选出“黑桃A”的信息量是多少？
Answer：
选出“黑桃A”的概率为

，则信息量为：

Q3：从52张扑克牌中，选出13张牌，获得“顺子”的信息量是多少？
Answer：
我不知道啥是顺子。。。

2.2

自信息的度量
自信息的计算公式如下所示：

2.3

信源编码

2.3.1

主要目的

压缩：使得信号能够更加有效地传输信息
纠错：使信号能够更加可靠的传输信息
–

现在的传输媒介有什么：
1.电线 2.光纤 3.无线电 (WIFI, 蓝牙) 4.空气 5.DNA 6.纸条 7.石头（三体）

2.3.2

均匀分布编码举例
手机

VIVO
小米
苹果
华为
则可以得到：
VIVO 小米 苹果 华为 的编码为：00 01 10 11

编码

00
01
10
11

2.3.3

不均匀分布编码
VIVO

50%

小米 25%
苹果 12.5%
华为 12.%

这时候，VIVO的出现概率更高，则有：
VIVO VIVO VIVO VIVO 小米 小米 苹果 华为 的编码为：00 00 00 00 01 01 10 11

信息编码太长，同时冗余度太高
此时修改编码方式：
VIVO

0

小米 10
苹果 110
华为 111

VIVO VIVO VIVO VIVO 小米 小米 苹果 华为 的编码为：0 0 0 0 10 10 110 111

相比之前节省了2个bit
则有二进制对称信道：

2.3.4

重复码

重复码，即

， 表示重复的次数。

下面的 表示噪声向量，接收向量

。

重复编码是一种简单的信道编码方法，其实质就是将每个要发送的符号重复发送，或者说是将原来的每一个信
源符号编成多个相同的码元符号，其值与原来的符号取值相同。比如(3，1)二元重复码，其编码方法就是将原来二进
制序列中的每一个“0”编成“000”，将每一个“1”编成“111”。
所谓的译码规则就是指接收符号与发送符号之间的映射关系。不同的译码规则会造成不同的平均错误概率，所
以人们一般都根据最小错误概率准则来确定译码规则。对于二元对称信道来说，一般总认为出错概率是小于等于0.5
的，所以对于二元重复码，最小错误概率准则与择多译码规则是一致的，也就是说，译码时根据码字中“0”“1”的数目
选择数目多的进行译码。比如(3，1)二元重复码的译码，可以将接收到的“000”、“001”、“010”和“100”译为“0”，将接收
到的“011”、“101”、“110”和“111”译为“1”。这样，每个码字对于传输过程中发生的任一位错误，通过译码都可以进行
自动纠正。可以证明，一个（n，1）重复码可以纠正传输过程中可能出现的不多于个差错。
得到的就是如下图所示的译码情况，通常用“服从多数”策略完成译码

计算 码的差错概率？
假设单个的出错概率为 ，

的差错概率是所有3个比特都出现翻转的概率

出现反转的概率

2.3.5

以及刚好两个比特

，则有：

汉明码
分组码是将长度为 的一个源比特序列 转换成长度为 比特的发送序列 的一条规则。
在线性分组码中，多出来的

奇偶校验比特。

比特是原有 比特的线性函数，这些多出来的比特叫做

汉明码，是分组码的一种，每收到

个比特，就发送

如上为例，将前四个发送比特设定为四个源比特

个比特。

，设定奇偶校验比特

使得每个圆中的奇偶校验为

偶校验，第一个奇偶校验比特是前三个源比特的奇偶校验（即如果这些比特之和是偶数，则奇偶校验比特的取值为
0，否则为1），后面如上图所示。

当出现错误时，纠错如下图所示：

译码的任务就是找出导致校验错误的反转比特的最小集合。
上图(b)：原先输入的正确结果是1000101，但是由于噪声：0100000，产生了错误结果：
汉明码

校正子为1，说明其中出现了

此时可以看到的是，在第一个圆中，一共有3个1，1个0，则在这个圆中的奇偶校验
比特错误，而同理，(b)中的圆的校正子为

，此时运行最优比特算法就能得到错误位置应该是在 ，需要将其

进行翻转。
上图(e)：当出现两个比特错误的时候，运行最优比特算法就有可能出现错误结果，如上图(e)其实错误的比特为
和 ，但是最后却把 给修正了。

3

自信息、二进制系统、熵的应用

3.1

香农信息量

在战舰游戏里，游戏双方都把舰队隐藏在用方格表示的海域中。每一轮游戏，一方朝对方海域中的一个方格开
火，试图击中对方的船只。

一次击中：

（击中的概率为

）

一次未击中，可得香农信息：
两次未击中，可得香农信息：
…….
连续32次未击中，可得总的香农信息量为：

连续32次未击中，可得总的香农信息量为：

如果第33次击中，则有

综上，无论什么时候击中，总的香农信息量是不变的：

香农信息量（self information）是一个客观的信息度量。
3.2

数字编码

如何编码100个数字？
1） 用100种形状的符号对应一个数字，那此时一个符号就代表一个数字，则一个符号代表的数
字即为

，则其信息量为

。

2） 用10种符号编码这十个数字，那么这十个符号之一可以代表的数字是

，此时其信息量为

，但是一个符号不能唯一编码一个数字，此时需要两两组合，当两两组合时，两个符
号的信息量加起来还是

。

上面的编码系统比较简单，但是编码长度更长了，比如说用十进制编码100个数字，只需要2个码长，但是如果
是二进制，码长就需要达到

，取整即7（

）。

由此可见，对数字的各种编码其实是等价的，无非是平衡编码复杂性和编码长度之间的关系。

3.3

信息熵的应用

作用——消除不确定性。

信息熵即信息的不确定性，其
信息熵具有可加性，即：

3.3.1

二进制熵函数

3.3.2

称重游戏

Q：给定12个球，其中11个球重量均等，而另一个球要么比其他球重，要么比其他球轻。你可以使用一架2-托盘
的天平。每次使用天平时，可将这些球中的一部分放在左托盘上，并取相同数目的球放在右托盘上，然后进行称
重。有三种可能的结果：重量相等，左边球重，或者左边球轻。你的任务就是设计一种方案，用尽量少的称重次数
找出哪一个球不同于其他球，并确定它是比其他球重还是轻。
第一次称重：
6

6：

5

5:

4

4：

3

3：

2

2：

1

1：

在上述图片中，1+代表的是1号小球为重的情况，1-则代表的是小球为轻的情况

假设首次结束以后，结果为

，此时假定

其他几种称重方式：
：
–

一半一半，概率很清楚
：

–

左倾：

–

右倾：

–

平衡：
：

–

左倾：说明球在轻的那三个里面，概率

–

右倾：不存在，因为H和L比已经是L轻了，不管L里面剩下那个球是不正常的
还是正常的，如果G和L比G更轻，就说明不止一个不正常小球

–

平衡：说明可能是L剩下的那个比较轻或者是H中4个比较重，所以概率是
：

–

左倾：说明球在轻的那三个里面，概率为

–

右倾：说明是选中的那个H偏重，概率为

–

平衡：剩下的L和三个H，就是你们了，概率为

大致上已经是明白了，以上图为例去���解释吧，这个是非信息论的解释，比如在12个球中，称重会出
现3种概率，左倾、右倾、平衡，即理想情况下三种情况的概率是1/3,1/3,1/3,则在12个球24种情况（球
X偏重/轻）中，最多只需要称量3次即可。这三种可能发生的概率越相近则越合适，于是在第一次称重
时，将其拆分为444才是最合适的称量方式，而在第二次的时候，同样也需要尽可能满足概率相似。以
左倾为例，在左倾时，只有332这种称量方式下，结果最为有效。

上面的

4

应该修改成

数据压缩
【回顾】通信系统的三大指标：
传输的有效性（压缩理论）
–

无损压缩：将所有的文件映射到不同的编码

–

有损压缩：将有些文件映射到相同的编码

传输的可靠性（传输理论）
传输的保密性（保密理论）

单随机变量的码字长度：随机变量X的可能结果数为
为

，对每个结果给定一个二进制码字，则每个码字的长度

。 的原始比特内容为：

多随机变量的码字长度：给定随机变量X和Y，一个有序对x, y的原始比特内容为：

有损压缩器：将有些不同的文件映射到相同的编码，此时会出现错误，用 表示源符号串是一个可混淆文件的概
率，即有损压缩器失效的概率。 不宜过大。比如：JPEG。
无损压缩器：将所有文件映射成不同的编码，如果它缩短了某些文件，就必定增长了其他文件。比如，霍夫曼
编码。

等长码压缩：
最小 充分子集
典型集：概率接近

的元素

香农信源编码原理：当

，可被压缩成多于

个比特，使得信息丢失的风险可以忽略不计

变长码压缩：
唯一可译码性（"Unique decodeability）: Kraft不等式
前缀码（Prefix codes）
最优符号码：

，

霍夫曼（Huffman）算法
算术码：概率小区间的计算，位长不足问题的解决

4.1

有损压缩

Q1：设
1）需要多少比特表示 的总体内容？
2）压缩方案是什么？
引入失效概率 表示元素 无对应编码的风险。
如果只对

进行编码，则失效概率

码：

压缩方案（构造
1.
2. 将

集合）

设为空集；
中的元素按照概率进行降序排序；

3. 将概率最大的元素开始加入

；

4. 判断总概率是否大于等于

，是则结束，否则跳转到2.

由此得到 的基本比特内容：

，这时候只需要两位编

最小 充分子集： 是满足
4.1.1

的最小

子集。

信源压缩方案

Q： 硬币彩票的例子：一个变形硬币以
的概率正面朝上，记为1，连续掷
次，产生的彩票结果
。
你可以从所有可能的 彩票中以1元的花费购买一张彩票。如果所买的彩票与硬币生成的序列一致，则会赢得1
000 000 000元。
问题1：如果必须购买一张彩票，那么应该买哪一种序列？
问题2：如何购买彩票，能保证99%的胜率？
Q1：必然买的是全0的序列了，因为为0的概率为0.9
Q2：引入函数
则

表示序列 中1的个数，

的概率服从二项分布

的期望值为：

的方差为：

根据经验主义，为了保证约

的胜率，

所以需要购买的彩票数量为：

典型集：
的方差为：
的标准差为：
随着N的增大， r的分布概率会越来越集中，此时r会落在一个很小的取值范围，这意味结果X也会相对应地落在
一个小的子集中，称为典型集。

N个符号所组成的长字符串通常包含：第一个符号约NP_1次，第二个符号约NP_2次，等等。因此这个符号串的
概率可以粗略估计为：

由此，一个典型字符串的信息量为：

定义

的典型元素是那些概率接近

的元素。典型集

为：

基于大数定理可以证明：无论选用什么样的β，随着N的增大，典型集几乎包含所有的概率。

渐进均���原理：
无论选用什么样的β，随着N的增大，典型集几乎包含所有的概率。
对于 个独立同分布随机变量的总体
定属于一个只有

个成员的子集，而每个元素的概率都“接近”

香农信源编码原理：当

，可被压缩成多于

来，如果压缩后的比特数少于

5

，当 足够大时，结果
。

个比特，使得信息丢失的风险可以忽略不计；反过

，几乎肯定会丢失信息。

符号码

符号码：将一个符号x映射到
此时给定符号串

5.1

。
，可以得到对应的二进制串

。

符号码性能分析
符号码的期望编码长度：

其中，

Q：

和

三大准则：

编码

唯一可译码性（unique

编码准确性：进行100%的准确性编码，即无损编码的要求，也称为
decodeability）；
编码简单性：

；

编码长度的简短性：

Q：

尽量小。

和

几乎肯

1）

的编码方式为

2）

的编码方式为

3）

的编码方式为

，问

，问

，问

但是不满足唯一可译码性捏
…）
6）

的编码方式为

，问

正好等于香农信息熵的值
并且

编码方式中无任何码字是其他码字的前缀，此类称为前缀码（prefix code），前缀码存

在两大优点：解码简单性和编码简单性。
7）

编码方式为：

，此时有：

直观的经验：出现概率高的符号给予短的编码，反之，出现概率低的符号给予长的编码。

5.2

克拉夫特不等式

克拉夫特（Kraft）不等式：唯一可译码性的充分非必要条件
对于 中每个字符，其二进制编码长度为
如果

那么该编码肯定不具有“唯一可译码性”。
如果：

也许具有“唯一可译码性”。

那么该编码

，

5.3

编码长度
理论上编码长度的下界：

已知

，则可知取下界时：

证明：给定一种字符集 ，及其对应的一种编码 ，其中 的编码长度为 ，可定义隐式变量(implicit probability)

此时，期望编码长度的度量：

Gibbs不等式：
并且等式在

时取得。

又因为

（因为具有唯一可译码性时

等式取得的条件为：
1.

;

），综上，

。

2. 完全编码。

KL散度（Kullback–Leibler divergence）又称KL距离，相对熵。当
5.4

Huffman算法

5.4.1

如何构建

和

的相似度越高，KL散度越小。

从低向上构建编码
从符号集中选取两个具有最小概率的符号
将这两个符号合并成一个符号，并继续重复下去，直至符号集中的元素为空

Q：给定字符集合

5.4.2

，及其对应的概率分布

缺点

变化的总体：Huffman码并不能很好地处理总体概率变化的情形。
2. 额外比特：Huffman码所到达的编码长度满足
1.

，当H(X)很大时，额外比特

并不重要，当H(X)很小时，额外比特不容忽视。

Huffman码的改善：
将多个字符打包，然后使用霍夫曼编码，需要确定数据块的数量，比如是 个字符打包，那么则需要
编码符号。

6
6.1

算术码
Shannon-Fano-Elias码

个待

: 符号的概率值；
: 概率累积函数(Cumulative Distribution Function, CDF)；

算术码长度：
上式中：

6.2

概率小区间

6.2.1

举例
给定字符集合

，及其对应的概率分布

，待编码的信源：

初始概率区间如下所示

首个字符串为 ，则取出
区间的

6.2.2

的 区间，第二个字符串为 ，则取出进一步取出已经取出

区间范围，一直往下取，最后得到的概率区间和待编码的信源唯一地对应。

概率小区间的计算
定义字符ai的发生概率为 ，
1. 划分初始区间（ 为 到 的求和）

2. 读入信源符号，在当前概率小区间上，得到嵌套收缩的新的概率小区间（

就是划分后的总长度）

Q：给定字符集���

，及其对应的概率分布

，待编码的信源：

1. 输入 ：

2. 输入 ：

3. 输入 ：

综上输出
6.2.3

，

即可表示

，解码过程就是编码过程的逆运算。

位长不足问题
this$is$his$hat$is$it$his$hat
字母表：
由此得到概率区间的划分：

，及其对应的出现频率：

。

Symbol
t

0.600000 0.733333 0.133333

h

0.608889 0.631111 0.022222

i

0.614074 0.618519 0.004444

s

0.616000 0.616741 0.000741

$

0.616543 0.616741 0.000198

i

0.616589 0.616629 0.000040

s

0.616606 0.616613 0.000007

$

0.616611 0.616613 0.000002

h

0.616611 0.616612 0.000000

i

0.616611 0.616612 0.000000

s

0.616611 0.616612 0.000000

$

0.616611 0.616612 0.000000

观察上表可以清楚地发现，在概率小区间相当小以后，限制于计算机的存储条件，最后左右一样，同时长度变为0

位长不足怎么解决？
获得当前小区间后，考察概率小区间左、右端点，从最高有效位开始，一旦发现二者的高有效
位匹配，就发送之；随后，概率小区间左、右端点进行相应变化。

6.3

算术码的特性

增量输出：流模式
最优性

松耦合的
较大概率的情况
没有那么直观：较为复杂
延时：解码需要时间
鲁棒性不强：一旦出差，全盘皆输
建模和编码是

适合字母表中存在

7

噪声信道编码

7.1

卡片游戏

Q：有三张卡片：甲卡片两面都是白色，乙卡片两面都是黑色，丙卡片一面是白色一面是黑色。从三张卡片中随
机抽取一张，并展示其中一面，若该面为白色。
问：另一面也是白色的概率是多少？
事件分析法
出现“该面为白色”的事件包括：甲卡片的一面，甲卡片的另一面， 丙卡片的白色面。因此，
“该面为白色”的意味： 甲卡片出现的概率为2/3，丙卡片出现的概率为1/3。因此，另一面也是白色
的概率是2/3，即B选项正确。
贝叶斯定理
令，B表示首面为白色的事件，A表示背面为白色的事件，因此基于贝叶斯定理，有：

│
其中，

7.2

，

，因此，

│

。

贝叶斯定理

其中𝑃(𝐴)为事件𝐴发生的概率，即先验概率； 𝑃(𝐴│𝐵)为事件𝐵发生后事件𝐴发生的条件概率，即后验概率；𝑃(𝐵|
𝐴)也称为似然度；𝑃(𝐵)也称为标准化常量。
因此，贝叶斯定理可以表示为：
后验概率=(似然度*先验概率)/标准化常量

Q：三扇门游戏

假如你在参加一个游戏，有三扇门1、2、3：其中有一扇门后面放着一辆汽车，另外两扇门后面是山羊，你会赢
得你选择的那扇门后面的礼物。游戏开始时，你任意选择一扇门，假如为门1。主持人从剩余两扇门中选择一扇后面
不是汽车的门打开，比如为门3，现在主持人问：为了赢得车，是否要改选门2（另外一扇没有被打开的门）？
A. 坚持原来的选择;
B. 更改选择;
C. 没有区别;
令，

表示汽车在𝐻门里的事件，

表示主持人打开门𝐷的事件。故事中

𝐷=3。
贝叶斯定理：

│

│
H

先验概率 P ( H )

似然度 P ( D = 3 | H )

后验概率 P ( H | D )

H=1

1/3

1/2

(1/3)*(1/2)/P(D)

H=2

1/3

1

(1/3)/P(D)

H=3

1/3

0

0

这里做一个简单的解释，首先参与者已经选择了打开门1，所以主持人是不会打开1的，主持人2选1的
情况下，当车在1中时，主持人开两扇门的情况是等可能的，而车在2时，主持人肯定会打开第三扇
门，而车在3时，主持人肯定不会打开第三扇门

│

7.3

一些概率知识

7.3.1

联合熵与边缘熵

边缘熵就是 和
X、Y的联合熵是：

。

独立随机变量的熵是可加的：

Q：计算

，

，

:

7.3.2

条件熵

条件熵，是指在给定某个数（某个变���为某个值）的情况下，另一个变量的熵是多少，变量的不确定性是多
少？

因为条件熵中X也是一个变量，意思是在一个变量X的条件下（变量X的每个值都会取），另一个变量Y熵对X的
期望。

Q：计算
望）：

（条件熵

定义为 给定条件下 的条件概率分布的熵对 的数学期

其余同理

后面就不算了

熵的全景图：

如下所示，是

上图显示了一个联合总体的总熵H(X,Y)是如何被分解的

7.4

信道编码

7.4.1

二进制对称信道

7.4.2

二进制删除信道

7.4.3

Z信道

7.4.4

有噪声打字机
有噪声打字机是由27个字母组成的集合，字母被排列成一个圆圈，当打字员输入B时，打出来的可能是A、B或

C，且每个均以1/3的概率出现，而输入为C时，输出为B、C或D，最后一个字母“-”与A相邻。

7.4.5

已知输出推断输入
如果假设从总体 中选择一条信道的输入 ，则得到一个联合总体

，其中随机变量 和 服从联合分布：

如果现在接收到一个特定符号 ，输入符号 为什么？可以用贝叶斯定理写出输入的后验分布：

Q：已知上图，令
么输入 的概率

息

，输入信号 的分布为：
为多少？

，

。如果观察到输出

，那

7.4.6

互信息

互信息熵

通信场景下，

表示给定输出𝒀情况下对输入𝑿的不确定性的减少，因此该

值越大，则越可以推断输入𝑿的情况。

互信息计算方式：
│
│
是多少？其定义是

对所有 的加权和，在二进制对称信道中，
，所以在二进制对称信道中，

第二种计算方式。首先计算 𝐻(𝑌)。

先使用

因此

是二进制熵函数：
由上，在对称二元信道模型中，易知
综上：

使用第一种计算方式。首先计算 𝐻(𝑋)。

由于

│

由上可知

，

综上：

7.4.6.1

互信息熵的值分布

已知（ 指的是

理论上：

的概率）：

的计算同理。

由上图可以观察到：

其中

7.5

时取到最大

信道容量

信道的容量定义为：

给定一个信道𝑄，该

其中

表示输入信号的分布，

此时计算互信息：

其中，

计算太麻烦了，懒得写了

所以：

表示达到最大互信息的输入信号的最优分布。

7.5.1

有噪声打字机的信道容量

计算：

计算：